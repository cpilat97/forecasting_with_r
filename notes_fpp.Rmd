---
title: "Chapter 1"
output: html_document
date: "2022-10-21"
---

```{r include=FALSE}
library(fpp3)
```

# Chapter 1

Predictability depends on:

1.  Understanding contributing factors
2.  Available data
3.  Similarity of future to past events
4.  can forecasts affect that being forecasted (self-fulfilling)

We need to be aware of our own limitations, not claiming more than possible

A key step in good forecasting is knowing the diff between what can be forecasted and whats essentially a coin flip.

Business forecasting is often mixed/muddled with goals & planning:

-   Forecasting is predicing the future as accurately as possible given X information.

-   Goals are what we'd like to happen. Should be linked to forecasts/plans, but are often set w/o any plan for achieving them.

-   Planning is a response to forecasts & goals...determining appropriate actions.

Determining what to forecast:

-   In the beginning of a project, decide what should be forecast & what the necessary components of a forecast will be.

Examples of time series forecasting:

-   annual profits

-   quarterly sales

-   monthly rainfall

-   weekly retail sales

-   Stock prices

-   Traffic counts

Time series relationships are not exact and will always contain some degree of error, which allows for some random variation and variables that may not be included in the model.

A normal predictor equation might look like: ED=f(current temperature, strength of economy, population,time of day, day of week, error)

whereas a time series model may look like:

$\text{ED}_{t+1} = f(\text{ED}_{t}, \text{ED}_{t-1}, \text{ED}_{t-2}, \text{ED}_{t-3},\dots, \text{error}),$

which allows future vars to be predicated based on past values of a var.

### Basic steps in forecasting tasks

1.  Problem Definition
    1.  Clearly defining the problem requires an understanding of the way forecasts will be used, who requires the forecasts, and how it will fit within the org. Forecasters need to spend time talking to everyone invoked in collecting data, maintaining databases, and who will be using the forecasts.
2.  Gathering Info
    1.  Stats data

    2.  Accumulated expertise of people collecting.

    3.  Occasionally old data is less useful due to changes in systems.
3.  Prelim Exploratory Analysis
    1.  Graph, look for patterns, significant trends? Seasonal Importance? Business cycles? Outliers?
4.  Choosing & Fitting models
    1.  Compare two or three potential models
5.  Using & Evaluating a forecast model
    1.  After model selection & parameter estimation, the performance can only be evaluated after periods of time are available.

### Forecasting Perspectives

-   We can think of the thing we're trying to forecast as a random var since it's unknown.

-   Generally, the variation associated w/ what we're forecasting shrinks as the event approaches (given X data, it's easier to have a more acc prediction for a month from now than a year).

-   Forecasts are usually accompanied by a prediction interval providing a range of vals that w/in an expected probability.

-   Point forecasts are the average of the possible future vals

# Chapter 2 - Time series graphs

### 2.1 `tsibble` objects

**Index variables**:

You can think of time series as a list of numbers, and can create tsibble objects just like a normal `tibble` object:

```{r}
y <- tsibble(
  Year = 2015:2019,
  Observation = c(123, 39, 78, 52, 110),
  index = Year
)
```

`tsibbles` build ontop of tibbles by seeting the index which creates temporal structures. In the above the year is the index. If you call `str()` on the object, you can see embedded s3 classes on the backend w/ a given attribute:

```{r}
str(y)
```

You can convert months into indexes calling the `yearmonth` func:

```{r yearmonth, include=TRUE, eval=FALSE}
z %>% 
  mutate(Month = yearmonth(Month)) %>% 
  as_tsibble(index = Month)
```

There are additional class functions depending on how your data is structured:

-   Annual `start:end`
-   Quarterly `yearquarter()`
-   Montholy `yearmonth()`
-   Weekly `yearweek()`
-   Daily `as_date()`, `ymd()`
-   sub-daily `as_datetime()`, `ymd_hms()`

**The index column is always the time index column** identify that column for the index.

**Key Variables:**

If you have specific key vars, like sex, you can set and store those in the tsibble object.

In the olympic swimmers dataset length & sex are the keys:

```{r}
olympic_running
```

`tsibble` objects can also take normal dplyr verbs & functions:

```{r}
PBS %>%
  filter(ATC2 == "A10") %>%
  select(Month, Concession, Type, Cost) %>%
  summarise(TotalC = sum(Cost)) %>%
  mutate(Cost = TotalC / 1e6) -> a10
```

**Converting to tsibbles**. you can convert normal data (ie data read from a db or csv) by calling `as_tsibble(key = keyvalues, index = time_index_value)`.

**For a tsibble to be valid, it requires a unique index for each combination of keys**

### 2.2 - Time Plots

The best place to start is with time plots which show a given point over time, for example:

```{r}
melsyd_economy <- ansett %>%
  filter(Airports == "MEL-SYD", Class == "Economy") %>%
  mutate(Passengers = Passengers/1000)
autoplot(melsyd_economy, Passengers) +
  labs(title = "Ansett airlines economy class",
       subtitle = "Melbourne-Sydney",
       y = "Passengers ('000)")
```

The `autoplot` command is really helpful to start getting plots going rather than trying to meticulously get things together yourself. While it might not produce a polished final graph for you, it's helpful in getting going and visualizing the data. You can glean a lot of useful information by looking at how the data varies over a given time period.

```{r}
autoplot(a10, Cost) +
  labs(y = "$ (millions)",
       title = "Australian antidiabetic drug sales")
```

### 2.3 Timer Series Patterns

1.  Trend - exists when there's longterm increases or decreases in data, doesn't have to be linear.
2.  Seasonal - Pattern occurs when a `ts` is affected by seasonal factors like time of year/day. Should be fixed & known period. Associated with the calendar and are fixed (holiday season/summer travel/school)
3.  Cyclic - rises & falls that are not tied to a fixed frequency. Usually at least two years. (bear/bull markets).

### 2.4 Seasonal Plots

Seasonal plots are like time plots e/c plotted against individual seasons.

```{r}
a10 %>% 
  gg_season(Cost, labels = "both") + 
  labs(y = "$", 
       title = "seasonal plot: antidiabetic drug sales")

```

(Note that seasons doesn't necessarily refer to the seasons of the weather. Seasons are any cyclical, expected changes throughout the year). Seasonal can be a daily cycle like with electricity - a peak during the afternoon, dips at midnight, etc...

```{r}
vic_elec %>% gg_season(Demand, period = "day") +
  theme(legend.position = "none") +
  labs(y="MWh", title="Electricity demand: Victoria")

vic_elec %>% gg_season(Demand, period = "week") +
  theme(legend.position = "none") +
  labs(y="MWh", title="Electricity demand: Victoria")
```

### 2.5 - Seasonal Sub-Queries

```{r}
a10 %>%
  gg_subseries(Cost) +
  labs(
    y = "$ (millions)",
    title = "Australian antidiabetic drug sales"
  )
```

You can show the seasonal plots per year per month. So you can see how certain things perform each month over the years.

```{r}
holidays <- tourism %>%
  filter(Purpose == "Holiday") %>%
  group_by(State) %>%
  summarise(Trips = sum(Trips))

autoplot(holidays, Trips) +
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")

gg_season(holidays, Trips) +
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")
```

### 2.6 - Scatterplots

You can use scatterplots to examine the relationship between two objects/series against one another.

```{r}
vic_elec %>%
  filter(year(Time) == 2014) %>%
  ggplot(aes(x = Temperature, y = Demand)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity demand (GW)")
```

Scatterplots are also a good way to start examining the correlation between objects - visually looking at the relationship (perceived or not) between two objects.

It's usefull to plot variables against one another in order to view/exmaine the relationship between vars and how they might correlate to one another.

```{r}
visitors <- tourism %>%
  group_by(State) %>%
  summarise(Trips = sum(Trips))
visitors %>%
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State), scales = "free_y") +
  labs(title = "Australian domestic tourism",
       y= "Overnight trips ('000)")
```

You can easily do this with the `ggally` package and create a correlation matrix against one another.

```{r}
visitors %>%
  pivot_wider(values_from=Trips, names_from=State) %>%
  GGally::ggpairs(columns = 2:9)
```

### 2.7 - Lag plots

Use the `gg_lag` function to create lag plots from your tsibble and the var you're looking at (in the example case, it's beer). Lag plots are essentially looking/examining the randomness of the data. So in the example there are two plots, lag 4 & 8, that are showing positive correlations.

```{r}
recent_production <- aus_production %>%
  filter(year(Quarter) >= 2000)
recent_production %>%
  gg_lag(Beer, geom = "point") +
  labs(x = "lag(Beer, k)")
```

### 2.8 - Autocorrelation

You can also use auto correlations to measure the linear relationship between lagged values:

```{r}
recent_production %>% ACF(Beer, lag_max = 9)
recent_production %>% ACF(Beer, lag_max = 9)

recent_production %>%
  ACF(Beer) %>%
  autoplot() + labs(title="Australian beer production")
```

acf here is referring to the vals in the 9 scatterplots & how correlated they are.

### 2.9 - White Noise

Time series with **NO** autocorrelation are considered white noise:

```{r}
set.seed(30)
y <- tsibble(sample = 1:50, wn = rnorm(50), index = sample)
y %>% autoplot(wn) + labs(title = "White noise", y = "")
```

### 2.10 - Exercises

# Chapter 3 - Time series decomp

### 3.1 - Transformations & Adjustments

There are four kinds of transformations that you can make to make dealing with time series a little bit easier: calendar adjustments, population adjustments, inflation adjustments and mathematical transformations. In general this is to make it simpler to deal with time series data and a bit more digestible.

-   Calendar Adjustments: Removing seasonal variation from a `TS` before doing any further analysis. ie, using the average sale of a given day when comparing monthly sales since months have different days which can affect total sales numbers.

-   Population Adjustments: Population adjustments are essentially changing the data from a population number to a per capital number (ie XX per 10000 people) to standardize the number of people in a dataset.

-   Inflation Adjustments: Inflation adjustments come into play when you're working with monetary data, as the price of items will increase over time due to inflation. In these cases, price index's will be used to compare prices against one another (ie you may state that all prices reflected are in 2000 money). The `Common Price Index` is the common one that most people utilize.

-   Mathematical Transformations: These are when you use mathematical formulas to transform data - log scales are a common example, power transformations (square or cubed-root transformations).

### 3.2 - Time series components

Additive decompositions are written as: yt=St+Tt+Rt\

### 3.3 - Moving Averages

Moving averages are used in classical decomposition and are used to estimate the trend-cycle. The general idea here is that we can get an estimate of the trend cycle at time *t* by averaging the values in a time series. Averages close in time are more likely to be related/a good approximation and so the averages eliminate some of the randomness that may occur. This is called m-**MA** or a moving average of order *m*. The following formula is used: m=2k+1\

```{r}
global_economy %>%
  filter(Country == "Australia") %>%
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Total Australian exports")
```

So if you wanted a 5-**MA**, that would be a 5 year moving average. This can be computed using the `slider_dbl()` function from the `slider` pckg:

```{r}
aus_exports <- global_economy %>%
  filter(Country == "Australia") %>%
  mutate(#2yrs before, current year, 2yrs after = 5yr MA
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE)
  )
```

Which can then be plotted:

```{r}
aus_exports %>%
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports") +
  guides(colour = guide_legend(title = "series"))
```

It's best to stick with odd numbers to keep the moving avg symmetrical, x years before, current year, x years after. You can also apply a moving average of a moving average in order; One reason for doing this is to make an even-order moving average symmetric:

```{r}
beer <- aus_production %>%
  filter(year(Quarter) >= 1992) %>%
  select(Quarter, Beer)
beer_ma <- beer %>%
  mutate(
    `4-MA` = slider::slide_dbl(Beer, mean,
                .before = 1, .after = 2, .complete = TRUE),
    `2x4-MA` = slider::slide_dbl(`4-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )
```

Note that to compute the 4-MA, we use only 1 year before and then 2 years after and then the 2-MA is only taking one year before. This brings symmetry back into the fold.

You're most likely to use moving averages when estimating the trend-cycle from seasonal data. For example, observe the following data:

```{r}
us_retail_employment_ma <- us_retail_employment %>%
  mutate(
    `12-MA` = slider::slide_dbl(Employed, mean,
                .before = 5, .after = 6, .complete = TRUE),
    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )
us_retail_employment_ma %>%
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = `2x12-MA`), colour = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

When you combine moving averages they become known as *weighted-moving averages*.

### 3.4 - Classical Decomposition 

The two forms of classical decomp are:

-   Additive decomp:

    ```{r}
    us_retail_employment %>%
      model(
        classical_decomposition(Employed, type = "additive")
      ) %>%
      components() %>%
      autoplot() +
      labs(title = "Classical additive decomposition of total
                      US retail employment")
    ```

-   Multiplicative decomp

### 3.5 - Other decomp methods

**X-11**- x-11 is was dev'd by the us census, and is fairly robust to outliers. It handles holiday effects and effects of known predictors.

**SEATS Method** - easonal Extraction in ARIMA Time Series

### 3.6 - STL Decomp
